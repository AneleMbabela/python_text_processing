{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ead390",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization refers to the converting a text string into indivudual tokens, Tokens many be words or punctations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01c24245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token List :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and']\n",
      "\n",
      " Total Tokens :  110\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# read the base file inot a rwa text variable\n",
    "base_file = open(os.getcwd()+ \"./datasets/Spark-Course-Description.txt\", 'rt')\n",
    "raw_text = base_file.read()\n",
    "base_file.close()\n",
    "\n",
    "# Extract tokens\n",
    "token_list = nltk.word_tokenize(raw_text)\n",
    "print(\"Token List : \",token_list[:20])\n",
    "print(\"\\n Total Tokens : \",len(token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4cd736",
   "metadata": {},
   "source": [
    "# Cleaning Text\n",
    "\n",
    "We will see examples of removing punctuation and convertiong to lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb0782",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "960468ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token List after removing punctuation :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'DevOps', 'specialists']\n",
      "\n",
      "Total tokens after removing punctuation :  100\n"
     ]
    }
   ],
   "source": [
    "# Use the punkt library to extract tokens\n",
    "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
    "print(\"Token List after removing punctuation : \",token_list2[:20])\n",
    "print(\"\\nTotal tokens after removing punctuation : \", len(token_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37054c9",
   "metadata": {},
   "source": [
    "### Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf72e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after converting to lower case :  ['in', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'devops', 'specialists']\n",
      "\n",
      "Total tokens after converting to lower case :  100\n"
     ]
    }
   ],
   "source": [
    "token_list3 = [word.lower() for word in token_list2 ]\n",
    "print(\"Token list after converting to lower case : \", token_list3[:20])\n",
    "print(\"\\nTotal tokens after converting to lower case : \", len(token_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40feae",
   "metadata": {},
   "source": [
    "# Stop word removal\n",
    "\n",
    "Removal stop words by using a standard stop word list available in NLTK for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c1ccaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\phumlani\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after removing stop words :  ['order', 'construct', 'data', 'pipelines', 'networks', 'stream', 'process', 'store', 'data', 'data', 'engineers', 'data-science', 'devops', 'specialists', 'must', 'understand', 'combine', 'multiple', 'big', 'data']\n",
      "\n",
      "Total tokens after removing stop words :  62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the standard stopward list\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stopwards\n",
    "token_list4 = list(filter(lambda token: token not in stopwords.words('english'), token_list3))\n",
    "print(\"Token list after removing stop words : \", token_list4[:20])\n",
    "print(\"\\nTotal tokens after removing stop words : \", len(token_list4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c784e348",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18e53daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after stemming :  ['order', 'construct', 'data', 'pipelin', 'network', 'stream', 'process', 'store', 'data', 'data', 'engin', 'data-sci', 'devop', 'specialist', 'must', 'understand', 'combin', 'multipl', 'big', 'data']\n",
      "\n",
      "Total tokens after Stemming :  62\n"
     ]
    }
   ],
   "source": [
    "# Use the PorterStemmer library for stemming.\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem data\n",
    "token_list5 = [stemmer.stem(word) for word in token_list4 ]\n",
    "print(\"Token list after stemming : \", token_list5[:20])\n",
    "print(\"\\nTotal tokens after Stemming : \", len(token_list5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab70e6f",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8ba4152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\phumlani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after Lemmatization :  ['order', 'construct', 'data', 'pipeline', 'network', 'stream', 'process', 'store', 'data', 'data', 'engineer', 'data-science', 'devops', 'specialist', 'must', 'understand', 'combine', 'multiple', 'big', 'data']\n",
      "\n",
      "Total tokens after Lemmatization :  62\n"
     ]
    }
   ],
   "source": [
    "# use the wordnet library to map words to their lemmatized form\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4]\n",
    "print(\"Token list after Lemmatization : \", token_list6[:20])\n",
    "print(\"\\nTotal tokens after Lemmatization : \", len(token_list6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8313221",
   "metadata": {},
   "source": [
    "### Comparison of tokens between raw, stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbbf3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw :  technologies , Stemmed :  technolog  , Lemmatized :  technology\n"
     ]
    }
   ],
   "source": [
    "# Check for token technologies\n",
    "print( \"Raw : \", token_list4[20], \", Stemmed : \", token_list5[20], \" , Lemmatized : \", token_list6[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72663eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
